{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n jobs for parallel tasks\n",
    "N_JOBS = -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "from draw_place import DrawPlace\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pickle.load(open( \"db.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "placement_hist = pickle.load(open(\"plot_hist.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da03d4d9877477c901d0c2c6b0288e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=60), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TXY = [np.stack(placement_hist[i]).T for i in tqdm(range(len(placement_hist)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 8434 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 9784 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 11234 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 12784 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 14434 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 16184 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 18034 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 19984 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 22034 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 24184 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 26434 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 28784 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 31234 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 33784 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done 36434 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done 39184 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 42034 tasks      | elapsed:   17.8s\n",
      "[Parallel(n_jobs=-1)]: Done 44984 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done 48034 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done 51184 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=-1)]: Done 54434 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done 57784 tasks      | elapsed:   24.2s\n",
      "[Parallel(n_jobs=-1)]: Done 61234 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 64784 tasks      | elapsed:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done 68434 tasks      | elapsed:   28.7s\n",
      "[Parallel(n_jobs=-1)]: Done 72184 tasks      | elapsed:   30.2s\n",
      "[Parallel(n_jobs=-1)]: Done 76034 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=-1)]: Done 79984 tasks      | elapsed:   33.4s\n",
      "[Parallel(n_jobs=-1)]: Done 84034 tasks      | elapsed:   35.0s\n",
      "[Parallel(n_jobs=-1)]: Done 88184 tasks      | elapsed:   36.8s\n",
      "[Parallel(n_jobs=-1)]: Done 92434 tasks      | elapsed:   38.5s\n",
      "[Parallel(n_jobs=-1)]: Done 96784 tasks      | elapsed:   40.3s\n",
      "[Parallel(n_jobs=-1)]: Done 101234 tasks      | elapsed:   42.1s\n",
      "[Parallel(n_jobs=-1)]: Done 105784 tasks      | elapsed:   44.0s\n",
      "[Parallel(n_jobs=-1)]: Done 110434 tasks      | elapsed:   46.2s\n",
      "[Parallel(n_jobs=-1)]: Done 115184 tasks      | elapsed:   48.4s\n",
      "[Parallel(n_jobs=-1)]: Done 120034 tasks      | elapsed:   50.7s\n",
      "[Parallel(n_jobs=-1)]: Done 124984 tasks      | elapsed:   53.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130034 tasks      | elapsed:   55.4s\n",
      "[Parallel(n_jobs=-1)]: Done 135184 tasks      | elapsed:   57.7s\n",
      "[Parallel(n_jobs=-1)]: Done 140434 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 145784 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 151234 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 156784 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 162434 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 168184 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 174034 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 179984 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 186034 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 192184 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 198434 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 204784 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 211234 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 217784 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 224434 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 231184 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 238034 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 244984 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 252034 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 254457 out of 254457 | elapsed:  1.8min finished\n"
     ]
    }
   ],
   "source": [
    "def split_nodes(i):\n",
    "    return np.stack([TXY[j][i] for j in range(len(TXY))]).astype('float64')\n",
    "traj_list = Parallel(n_jobs=N_JOBS, verbose=1, backend=\"threading\")(delayed(split_nodes)(i) for i in range(TXY[0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25f0f2f521e4f00ae5590de31c0924a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=254457), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_d = np.stack([x.flatten() for x in tqdm(traj_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "branch_factor = 100\n",
    "levels = 2\n",
    "noise_tolerance = 1000\n",
    "\"\"\"\n",
    "cluster_mat = np.zeros((X_d.shape[0],levels))\n",
    "\n",
    "idx_map = np.zeros((X_d.shape[0],levels))\n",
    "\n",
    "X_d_list = [(X_d,)]\n",
    "for i in tqdm(range(levels)):\n",
    "    X_d_list_tmp = []\n",
    "    for X_di, imap in X_d_list:\n",
    "        k_means = KMeans(branch_factor, n_jobs = N_JOBS).fit(X_di)\n",
    "        labels = np.unique(k_means.labels_)\n",
    "        for label in labels.tolist():\n",
    "            l_idx = np.where(k_means.labels_ == label)\n",
    "            cluster_mat[i,imap][l_idx] = label\n",
    "            X_d_list_tmp.append(X_d[i][l_idx])\n",
    "    X_d_list = X_d_list_tmp\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.cluster.vq import *\n",
    "import pickle\n",
    "\n",
    "\n",
    "class tree(object):\n",
    "\tdef __init__(self, name, data=None, additional_data=None, children=None):\n",
    "\t\tself.name = name\n",
    "\t\tself.data = data\n",
    "\t\tself.additional_data = additional_data\n",
    "\t\tif children is None:\n",
    "\t\t\tself.children = []\n",
    "\t\telse:\n",
    "\t\t\tself.children = children\n",
    "\n",
    "\tdef set_data(self, data):\n",
    "\t\tself.data = data\n",
    "\n",
    "\tdef get_data(self):\n",
    "\t\treturn self.data\n",
    "\n",
    "\tdef set_additional_data(self, additional_data):\n",
    "\t\tself.additional_data = additional_data\n",
    "\n",
    "\tdef get_additional_data(self):\n",
    "\t\treturn self.additional_data\n",
    "    \n",
    "\tdef set_name(self, name):\n",
    "\t\tself.name = name\n",
    "\n",
    "\tdef get_name(self):\n",
    "\t\treturn self.name\n",
    "\n",
    "\tdef set_children(self, children):\n",
    "\t\tself.children = children\n",
    "\n",
    "\tdef add_child(self, child):\n",
    "\t\tself.children.append(child)\n",
    "\n",
    "\tdef add_children(self, children):\n",
    "\t\tself.children.extend(children)\n",
    "\n",
    "\tdef get_children_number(self):\n",
    "\t\treturn len(self.children)\n",
    "\n",
    "\tdef gather_data(self, gather_additional_data=False):\n",
    "\t\t'''Gather data from its children, depth=1'''\n",
    "\t\tif gather_additional_data:\n",
    "\t\t\treturn [child.get_additional_data() for child in self.children]\n",
    "\t\telse:\n",
    "\t\t\treturn [child.get_data() for child in self.children]\n",
    "\n",
    "\tdef find(self, data, find_additional_data=False):\n",
    "\t\tif find_additional_data:\n",
    "\t\t\tif self.additional_data == data:\n",
    "\t\t\t\treturn self\n",
    "\t\t\telse:\n",
    "\t\t\t\tfor child in self.children:\n",
    "\t\t\t\t\tres = child.find(data, find_additional_data)\n",
    "\t\t\t\t\tif res != -1:\n",
    "\t\t\t\t\t\treturn res\n",
    "\t\t\t\treturn -1\n",
    "\t\telse:\n",
    "\t\t\tif self.data == data:\n",
    "\t\t\t\treturn self\n",
    "\t\t\telse:\n",
    "\t\t\t\tfor child in self.children:\n",
    "\t\t\t\t\tres = child.find(data, find_additional_data)\n",
    "\t\t\t\t\tif res != -1:\n",
    "\t\t\t\t\t\treturn res\n",
    "\t\t\t\treturn -1\n",
    "\n",
    "\tdef is_leaf_node(self):\n",
    "\t\treturn len(self.children) == 0\n",
    "\n",
    "\tdef gather_leaves(self, output):\n",
    "\t\tassert type(output) == list, 'output must be a list.'\n",
    "\t\tif self.is_leaf_node():\n",
    "\t\t\toutput.append(self)\n",
    "\t\t\treturn\n",
    "\t\tfor child in self.children:\n",
    "\t\t\tchild.gather_leaves(output)\n",
    "\n",
    "\tdef gather_data_from_leaves(self, output, gather_additional_data=False):\n",
    "\t\tassert type(output) == list, 'output must be a list.'\n",
    "\t\tif gather_additional_data:\n",
    "\t\t\tif self.is_leaf_node():\n",
    "\t\t\t\toutput.append(self.additional_data)\n",
    "\t\t\t\treturn\n",
    "\t\t\tfor child in self.children:\n",
    "\t\t\t\tchild.gather_data_from_leaves(output, gather_additional_data)\n",
    "\t\telse:\n",
    "\t\t\tif self.is_leaf_node():\n",
    "\t\t\t\toutput.append(self.data)\n",
    "\t\t\t\treturn\n",
    "\t\t\tfor child in self.children:\n",
    "\t\t\t\tchild.gather_data_from_leaves(output, gather_additional_data)\n",
    "\n",
    "\tdef __str__(self):\n",
    "\t\treturn str(self.name)\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn str(self)\n",
    "\n",
    "\tdef __iter__(self):\n",
    "\t\treturn iter(self.children)\n",
    "\n",
    "\tdef __hash__(self):\n",
    "\t\treturn hash(str(self.name))\n",
    "\n",
    "\n",
    "class hierarchical_kmeans(object):\n",
    "\tdef __init__(self, clusters):\n",
    "\t\tself.clusters = clusters  # tree type variable\n",
    "\t\tself.root = tree('root')\n",
    "\t\tself.total_k = 0\n",
    "\n",
    "\tdef cluster(self, data, only_store_id=True, iteration=1):\n",
    "\t\tdata = np.asarray(data)\n",
    "\t\tself.only_store_id = only_store_id\n",
    "\t\tcluster = self.clusters\n",
    "\t\tcurrent_node = self.root\n",
    "\t\tidx = np.arange(data.shape[0])\n",
    "\t\tprint('Doing hierarchical kmeans...')\n",
    "\t\tself._cluster(data, idx, current_node, cluster, iteration=iteration, only_store_id=only_store_id)\n",
    "\t\tprint('Done! Actual total number of clusters is %d.' % self.total_k)\n",
    "\n",
    "\tdef _cluster(self, data, idx, current_node, cluster_node, iteration=1, only_store_id=True):\n",
    "\t\tif cluster_node.is_leaf_node():\n",
    "\t\t\tself.total_k += 1\n",
    "\t\t\tif only_store_id:\n",
    "\t\t\t\tcurrent_node.set_additional_data(idx)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcurrent_node.set_additional_data(data[idx])\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tcodebook, _ = kmeans(data[idx], cluster_node.get_children_number(), iter=iteration)\n",
    "\t\tids, _ = vq(data[idx], codebook)\n",
    "\n",
    "\t\tif codebook.shape[0] == 1:  # kmeans only find one cluster\n",
    "\t\t\tcurrent_node.set_data(codebook[0])\n",
    "\t\t\tif only_store_id:\n",
    "\t\t\t\tcurrent_node.set_additional_data(idx)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcurrent_node.set_additional_data(data[idx])\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tfor i, c in enumerate(cluster_node):\n",
    "\t\t\tif i >= codebook.shape[0]:  # cluster number smaller than the number of next branch\n",
    "\t\t\t\tbreak\n",
    "\t\t\tidx_i = idx[np.nonzero(ids == i)[0]]\n",
    "\t\t\tbranch = tree(c.get_name(), codebook[i])\n",
    "\t\t\tcurrent_node.add_child(branch)\n",
    "\t\t\tself._cluster(data, idx_i, branch, c, iteration=iteration, only_store_id=only_store_id)\n",
    "\n",
    "\tdef save(self, file_name):\n",
    "\t\twith open(file_name, 'wb') as f:\n",
    "\t\t\tpickle.dump([self.clusters, self.root, self.total_k], f)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef load(file_name):\n",
    "\t\thk = hierarchical_kmeans(None)\n",
    "\t\twith open(file_name, 'rb') as f:\n",
    "\t\t\t[hk.clusters, hk.root, hk.total_k] = pickle.load(f)\n",
    "\t\treturn hk\n",
    "    \n",
    "\tdef prune(self, eps, max_depth=-1):\n",
    "\t\tself._prune(eps, self.root, 0, max_depth)\n",
    "    \n",
    "\tdef _prune(self, eps, current_node, current_depth, max_depth=-1):\n",
    "\t\tif current_node.is_leaf_node():\n",
    "\t\t\tcodebook = current_node.gather_data()\n",
    "\t\t\tcurrent_depth += 1\n",
    "\t\t\tif len(codebook) < eps:\n",
    "\t\t\t\tcurrent_node.set_name('noise')\n",
    "\t\t\t\tcurrent_node.set_children([])\n",
    "        \n",
    "\t\tfor i, c in enumerate(current_node):\n",
    "\t\t\tself._prune(eps, c, current_depth, max_depth)\n",
    "\n",
    "\tdef find_cluster(self, data, max_depth=-1):\n",
    "\t\tdata = np.asarray(data)\n",
    "\t\tif len(data.shape) == 1:\n",
    "\t\t\treturn self._find_cluster([data], self.root, 0, max_depth)\n",
    "\t\treturn [self._find_cluster([d], self.root, 0, max_depth) for d in data]\n",
    "\n",
    "\tdef _find_cluster(self, data, current_node, current_depth, max_depth=-1):\n",
    "\t\tif current_node.is_leaf_node() or (max_depth >= current_depth and max_depth > 0):\n",
    "\t\t\treturn current_node\n",
    "\t\tcodebook = current_node.gather_data()\n",
    "\t\tid, _ = vq(data, codebook)\n",
    "\t\tcurrent_depth += 1\n",
    "\t\tbranch = current_node.children[id[0]]\n",
    "\t\treturn self._find_cluster(data, branch, current_depth, max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254457, 120)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing hierarchical kmeans...\n",
      "Done! Actual total number of clusters is 999.\n"
     ]
    }
   ],
   "source": [
    "cluster_structure = tree('clust_test')\n",
    "for i in range(branch_factor):\n",
    "    x = tree('l1-c%d' % (i+1))\n",
    "    cluster_structure.add_child(x)\n",
    "    for j in range(10):\n",
    "        y = tree('l2-c%d-c%d' % (i+1, j+1))\n",
    "        x.add_child(y)\n",
    "hk = hierarchical_kmeans(cluster_structure)\n",
    "hk.cluster(X_d, only_store_id=True, iteration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk.prune(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noise'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hk.root.children[0].children[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8e77ac39f34e32ae56beb6835aa4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=254457), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#hk.find_cluster(X_d[0,:])\n",
    "lab = [hk.find_cluster(X_d[i,:]).get_name() for i in tqdm(range(X_d.shape[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'noise' in lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254457, 254457)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in lab if x == 'noise']), len(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2idx = list(set(lab))\n",
    "labid = [id2idx.index(x) for x in tqdm(lab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labid = np.array(labid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(labid == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(k_means.labels_ == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draw_place import DrawPlace\n",
    "d = DrawPlace(db)\n",
    "#clust2col = [list(np.random.choice(range(256), size=3)/255) for _ in range(np.unique(k_means.labels_).shape[0])]\n",
    "\n",
    "#for i in tqdm(range(len(placement_hist))):\n",
    "#    d.forward(np.concatenate([placement_hist[i][0],placement_hist[i][1]]), (k_means.labels_).astype(np.int), clust2col, 'plots/plot_'+str(i)+str('.png'))\n",
    "clust2col = [list(np.random.choice(range(256), size=3)/255) for _ in range(np.unique(labid).shape[0])]\n",
    "\n",
    "for i in tqdm(range(len(placement_hist))):\n",
    "    d.forward(np.concatenate([placement_hist[i][0],placement_hist[i][1]]), (labid).astype(np.int), clust2col, 'plots/'+str(i)+str('.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
